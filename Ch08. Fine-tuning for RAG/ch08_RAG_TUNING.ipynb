{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d31a264-5523-46f8-be31-732b986cb3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.4.0\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting transformers==4.45.1\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets==3.0.1\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate==0.34.2\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl==0.11.1\n",
      "  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting peft==0.13.0\n",
      "  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.4.0)\n",
      "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.45.1)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.45.1)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.1)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.45.1)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0 (from datasets==3.0.1)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.0.1)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==3.0.1)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from transformers==4.45.1)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets==3.0.1)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp (from datasets==3.0.1)\n",
      "  Downloading aiohttp-3.11.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (5.9.6)\n",
      "Collecting tyro>=0.5.11 (from trl==0.11.1)\n",
      "  Downloading tyro-0.9.17-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading multidict-6.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fsspec[http]<=2024.6.1,>=2023.1.0 (from datasets==3.0.1)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2022.12.7)\n",
      "Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (2.1.2)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==3.0.1)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==3.0.1)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.1) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (2.16.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m164.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.11.1-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m154.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m132.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m133.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.17-py3-none-any.whl (123 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.7/123.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m131.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, typing-extensions, triton, tqdm, shtab, safetensors, requests, regex, pyarrow, propcache, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mdurl, fsspec, frozenlist, docstring-parser, dill, async-timeout, aiohappyeyeballs, typeguard, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, multidict, markdown-it-py, huggingface-hub, aiosignal, yarl, tokenizers, rich, nvidia-cusolver-cu12, tyro, transformers, torch, aiohttp, accelerate, peft, datasets, trl\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.34.2 aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.0.1 dill-0.3.8 docstring-parser-0.16 frozenlist-1.5.0 fsspec-2024.6.1 huggingface-hub-0.29.3 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.2.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.1.105 pandas-2.2.3 peft-0.13.0 propcache-0.3.1 pyarrow-19.0.1 pytz-2025.2 regex-2024.11.6 requests-2.32.3 rich-13.9.4 safetensors-0.5.3 shtab-1.7.1 tokenizers-0.20.3 torch-2.4.0 tqdm-4.67.1 transformers-4.45.1 triton-3.0.0 trl-0.11.1 typeguard-4.4.2 typing-extensions-4.13.0 tyro-0.9.17 tzdata-2025.2 xxhash-3.5.0 yarl-1.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.4.0 transformers==4.45.1 datasets==3.0.1 accelerate==0.34.2 trl==0.11.1 peft==0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39c2efe4-18e5-4957-aa3e-ef377ed7a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f8a330a-3ab7-4b13-bdcf-bb1c482c0ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터의 type 분포:\n",
      "no_answer: 404\n",
      "mrc_question: 491\n",
      "paraphrased_question: 196\n",
      "synthetic_question: 497\n",
      "mrc_question_with_1_to_4_negative: 296\n",
      "\n",
      "전체 데이터 분할 결과: Train 380개, Test 1504개\n",
      "\n",
      "학습 데이터의 type 분포:\n",
      "no_answer: 81\n",
      "mrc_question: 99\n",
      "paraphrased_question: 40\n",
      "synthetic_question: 100\n",
      "mrc_question_with_1_to_4_negative: 60\n",
      "\n",
      "테스트 데이터의 type 분포:\n",
      "no_answer: 323\n",
      "mrc_question: 392\n",
      "paraphrased_question: 156\n",
      "synthetic_question: 397\n",
      "mrc_question_with_1_to_4_negative: 236\n"
     ]
    }
   ],
   "source": [
    "# 허깅페이스 허브에서 데이터셋 로드 ← ❶\n",
    "dataset = load_dataset(\"iamjoon/klue-mrc-ko-rag-dataset\", split=\"train\")\n",
    "\n",
    "# system_message 정의 ← ❷\n",
    "system_message = \"\"\"당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
    "\n",
    "다음의 지시사항을 따르십시오.\n",
    "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
    "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
    "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\"라고 답변하십시오.\n",
    "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
    "5. 예를 들어 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]라고 기재하십시오.\n",
    "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
    "\n",
    "검색 결과:\n",
    "-----\n",
    "{search_result}\"\"\"\n",
    "\n",
    "# 원본 데이터의 type별 분포 출력 ← ❸\n",
    "print(\"원본 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    print(f\"{type_name}: {dataset['type'].count(type_name)}\")\n",
    "\n",
    "# train/test 분할 비율 설정 (0.5면 5:5로 분할) ← ❹\n",
    "test_ratio = 0.8\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# type별로 순회하면서 train/test 데이터 분할 ← ❺\n",
    "for type_name in set(dataset['type']):\n",
    "    # 현재 type에 해당하는 데이터의 인덱스만 추출\n",
    "    curr_type_data = [i for i in range(len(dataset)) if dataset[i]['type'] == type_name]\n",
    "    \n",
    "    # test_ratio에 따라 test 데이터 개수 계산 \n",
    "    test_size = int(len(curr_type_data) * test_ratio)\n",
    "    \n",
    "    # 현재 type의 데이터를 test_ratio 비율로 분할하여 추가\n",
    "    test_data.extend(curr_type_data[:test_size])\n",
    "    train_data.extend(curr_type_data[test_size:])\n",
    "\n",
    "# OpenAI format으로 데이터를 변환하기 위한 함수 ← ❻\n",
    "def format_data(sample):\n",
    "    # 검색 결과를 문서1, 문서2... 형태로 포매팅\n",
    "    search_result = \"\\n-----\\n\".join([f\"문서{idx + 1}: {result}\" for idx, result in enumerate(sample[\"search_result\"])])\n",
    "    \n",
    "    # OpenAI format으로 변환\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message.format(search_result=search_result),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample[\"question\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": sample[\"answer\"]\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "# 분할된 데이터를 OpenAI format으로 변환 ← ❼\n",
    "train_dataset = [format_data(dataset[i]) for i in train_data]\n",
    "test_dataset = [format_data(dataset[i]) for i in test_data]\n",
    "\n",
    "# 최종 데이터셋 크기 출력 ← ❽\n",
    "print(f\"\\n전체 데이터 분할 결과: Train {len(train_dataset)}개, Test {len(test_dataset)}개\")\n",
    "\n",
    "# 분할된 데이터의 type별 분포 출력 ← ❾\n",
    "print(\"\\n학습 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    count = sum(1 for i in train_data if dataset[i]['type'] == type_name)\n",
    "    print(f\"{type_name}: {count}\")\n",
    "\n",
    "print(\"\\n테스트 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    count = sum(1 for i in test_data if dataset[i]['type'] == type_name)\n",
    "    print(f\"{type_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "887da04f-c095-46fd-8970-71a0d1cab977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\\n\\n다음의 지시사항을 따르십시오.\\n1. 질문과 검색 결과를 바탕으로 답변하십시오.\\n2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\\n3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\"라고 답변하십시오.\\n4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\\n5. 예를 들어 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]라고 기재하십시오.\\n6. 최대한 다수의 문서를 인용하여 답변하십시오.\\n\\n검색 결과:\\n-----\\n문서1: LED(발광다이오드) 조명 등을 만드는 동부그룹 계열사 동부라이텍은 일본 요코하마에 LED 라이트 패널(루미시트) 생산공장을 완공, 본격 양산에 들어갔다고 31일 발표했다. 이 공장은 일본 현지 유통사인 테크타이토와 합작해 세운 공장이다. 루미시트는 얇은 종이판 형태의 LED 조명으로, 이 공장에서는 광고 인테리어용 루미시트 4종을 양산한다.동부라이텍은 2008년 캐나다 토론토에 현지 합작법인 DLC를 세워 북미 고급 매장에서 사용하는 진열대용 루미시트를 생산하고 있다. DLC는 올해 상반기에 약 200억원의 매출을 올렸고, 순이익은 최근 수년간 매년 20%씩 증가하고 있다. 요코하마 공장은 캐나다에서의 성공 모델을 일본으로 옮겨온 것이라는 게 회사 측 설명이다. 동부라이텍은 테크타이토와 합작해 지난해 8월 도쿄에 자본금 1억엔 규모의 합작법인 씨엔디라이텍을 설립한 뒤 현지 공장 가동을 준비해 왔다. 동부라이텍은 테크타이토의 일본 내 유통망을 활용해 일본 루미시트 시장에서 점유율을 늘릴 수 있을 것으로 기대하고 있다.'},\n",
       " {'role': 'user', 'content': '테크타이토와 합작해서 지은 공장은 어느 지역에 있는가?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '테크타이토와 합작해서 지은 공장은 일본 요코하마에 있습니다 [[ref1]].'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[345][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "029fc179-c5af-4b84-a828-2e531c298f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# 리스트 형태에서 다시 Dataset 객체로 변경\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6493c800-ac29-46fd-af4f-e840278ffc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b82d1d1dee41538300b971b13836c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4557a1ce9ed44b00920438d3ea2e0ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e8a302a7db446fa6d82dd1809476d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48ec0334f5941d4be88fe427a90b8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591d71fb88a24051a6cb715ef3818c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923b7bebb3cc49e99615392297ef0f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0612f4509a43ab9cb5c76fe4021057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64206b7d1b3d4133b2616921201637d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5555d82e0dd47e589ee1f51b9f1cb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b722a69a6ca4907bc85bef2b5fcf6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c6b8c7f57f4da4a01500ca7eaa7e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a523fc43774a62b7a22a2651dbd9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17aa3df6012447678d6b4ad1343d3f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 허깅페이스 모델 이름\n",
    "model_id = \"Qwen/Qwen2-7B-Instruct\" \n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34d058cd-15f9-4764-8dc7-876591f5b700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
      "\n",
      "다음의 지시사항을 따르십시오.\n",
      "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
      "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
      "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\"라고 답변하십시오.\n",
      "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
      "5. 예를 들어 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]라고 기재하십시오.\n",
      "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
      "\n",
      "검색 결과:\n",
      "-----\n",
      "문서1: 콘코바르는 결국 죽게 되는데, 그 곡절은 이러하다. 라긴의 왕 메스 게그러의 뇌를 굳힌 겻을 울라의 코날이 전리품으로 가지고 있었는데, 코나크타의 전사 케트 막 마가크가 이를 훔쳐갔다. 그리고 케트는 무릿매로 메스 게그러의 뇌를 던져 콘코바르의 머리를 맞추었고, 메스 게그러의 뇌가 콘코바르의 머리통 깊숙히 박혀 버렸다. 이 일이 일어난 곳은 우르카르(Urchair)의 발러 아흐(Baile Ath), 곧 오늘날의 웨스트미스 주 호르셀리프라고 한다. 콘코바르의 의사들은 이 이물질을 제거할 수 없었고, 상처를 봉합만 한 뒤 왕에게 흥분하지 않으면 생명을 유지할 수 있다고 말했다. 7년이 평화롭게 흘러간 뒤 콘코바르는 그리스도가 죽었다는 소식을 듣게 되어 분노했고, 뇌가 터져 죽었다. 머리가 터진 자리에서 뿜어져나온 피의 세례를 받은 결과 그는 기독교인이 되었고 그 영혼은 천국으로 갔다. 콘코바르의 죽음에 관한 이 기록은 매우 얄팍한 기독교화가 이루어져 있는데, 한편 노르드 신화의 토르가 흐룽그니르와 싸우다 머리에 숫돌이 박힌 이야기와 유사한 점이 있다. 어쩌면 두 이야기는 하나의 기원을 공유하거나 또는 바이킹이 에린에 영향을 미치던 시기(노르드-게일)에 수입된 것일 수도 있다.\n",
      "\n",
      "울라인들은 아직도 코나크타에 망명 중이던 콘코바르의 아들 코르막 콘드 롱가스를 왕으로 추대하기 위해 초빙했는데, 이멘마하로 돌아오는 길에 코르막은 기아스를 어기게 되었고 다 코카(Da Choca) 여관에 묵던 중에 습격당해 피살당했다. 이에 코날 케르나크가 콘코바르의 다른 아들 쿠스라드 멘드 마하(Cúscraid Mend Macha)를 왕으로 추대했다.\n",
      "-----\n",
      "문서2: 케트 막 마가크(Cet mac Mágach)는 아일랜드 신화의 얼스터 대계에 등장하는 인물이다. 코나크타의 전사로, 울라의 코날 케르나크의 적수이다.\n",
      "\n",
      "어떤 전승들에서는 케트가 코날의 어머니 핀드코엠과 남매지간, 코날과는 숙질간이라고도 한다.\n",
      "\n",
      "라긴 사람 막 다 호의 집에서 연회가 벌어졌을 때, 코나크타와 울라의 여러 전사들이 쿠라드미르를 놓고 각자의 업적을 자랑하며 다투었다. 케트는 전사들 각각을 자신이 어떻게 이겼는지 들먹이며 그들 모두의 입을 다물게 만들었다. 케트가 쿠라드미르를 가져가려는 순간 코날 케르나크가 도착했다. 코날은 자신이 케트보다 우위에 있다고 주장했고, 케트는 패배를 인정했다. 그러나 자신의 동생 아늘룬이 있었다면 그가 코날의 우위에 있었을 것이라고 주장했다. 그러자 코날은 방금 잘라온 아늘룬의 머리를 케트에게 던져주는 것으로 응수했다. \n",
      "\n",
      "케트는 울라의 왕 콘코바르 막 네사를 죽였는데, 그 이야기는 이러하다. 코날이 라긴의 왕 메스 게그러를 죽이고 그 뇌를 굳힌 것을 트로피삼아 차고 다녔는데, 케트가 그것을 훔쳐다가 무릿매로 던져 콘코바르의 머리통에 파묻었다. 콘코바르의 의사들은 왕을 죽이지 않는 이상 이 이물질을 빼낼 방법을 찾지 못했다. 그래서 대충 봉합한 뒤 왕에게 지나치게 흥분하지 않는다면 목숨을 부지할 수 있다고 말했다. 7년이 평화롭게 흘러간 끝에 콘코바르는 그리스도가 죽었다는 소식을 듣게 되었다. 그는 분노했고, 그 바람에 뇌가 터져 죽었다.\n",
      "\n",
      "어느 겨울날 케트가 울라로 습격을 나가 울라 남자 스물일곱 명을 죽이고 그 수급들을 베어갔다. 눈이 내렸기에 코날은 케트의 흔적을 추적할 수 있었다. 코날은 케트를 따라잡았지만 그를 대적하기 주저했는데, 전차를 몰던 마부가 그를 겁쟁이라 비난하자 마음을 고쳐먹고 나서게 되었다. 둘이는 한 여울에서 일 대 일 결투를 벌였고, 격렬한 싸움 끝에 코날이 케트를 죽였으며 코날 본인도 빈사상태에 빠졌다.\n",
      "-----\n",
      "문서3: 뉴욕의 광고 회사에서 일하는 테드 크레이머는 능력 있는 일꾼이지만 워커홀릭이어서 가정에는 소홀한 가장이다. 아내 조애나는 테드의 귀가가 늦은 어느 날 갑작스럽게 이유를 밝히지 않고 이혼을 통보한 뒤 집을 나가 버린다. 테드는 애써 한순간의 변덕이고 금방 돌아오겠지 하는 생각은 하지만, 곧 아들 빌리를 돌보면서 서툰 집안일을 해야 할 처지가 된다. 일과 양육을 병행하느라 회사에서의 평판은 나빠지고, 빌리는 계속 해서 이어지는 엄마의 부재 때문에 짜증을 낸다. 다행히도 시간이 지나면서 부자는 둘만의 생활에 적응해 나간다.\n",
      "\n",
      "테드는 아랫집에 사는 이웃이자 조애나와 친했던 마거릿과 친해진다. 마거릿도 테드와 마찬가지로 이혼 후 자식들 혼자 키우고 있었으므로 둘은 서로의 처지에 공감했던 것이다. 하루는 잠시 한눈을 팔던 사이 빌리가 정글짐에서 추락해 머리가 찢어지는 사건이 벌어진다. 위급한 테드는 아들을 안고 먼 거리를 달려 병원에 데려가고, 부성애를 발휘해 아들의 수술을 지켜보며 아들을 안심케 한다. 수술 후 테드는 마거릿을 빌리의 대모로 삼는다.\n",
      "\n",
      "조애나가 떠난 지 15개월 만에 뉴욕에 돌아와 테드와 만난다. 둘은 처음에는 친밀했다. 조애나는 자기가 떠난 것은 아들을 키울 준비를 하기 위해서였으며, 이제 아들의 양육권을 달라고 한다. 그 말을 들은 테드는 크게 화를 내며 자리를 박차고 나선다. 변호사가 고용되고 곧 법정 다툼으로 발전한다. 불행히도 재판을 앞두고 테드가 직장에서 해고되는 바람에 양육권 재판에 불리하게 변하자, 테드는 연봉을 크게 낮춰가면서까지 새 직장을 구한다.\n",
      "\n",
      "조애나는 재판에서 결혼 생활은 원만하지 않았고 테드의 일 중독과 편견 때문에 자신은 패션디자이너로서의 꿈을 접어야 했다고 증언한다. 지금은 직업을 얻어 테드보다도 더 연봉이 높고 가정에 소홀했던 테드보다 자신이 빌리를 키우기에 더 적합하다고 말한다. 테드는 조애나의 고백에 마음이 움직이지만, 변호사가 결혼 생활의 파탄 원인은 조애나라고 몰아붙여 재판은 치열해진다. 두 번째 재판에서, 마거릿이 증인으로 나와 테드가 변했다고 옹호하지만, 조애나의 변호사가 조애나를 떠나도록 유도한 것은 바로 마거릿이었음을 밝히며 설득력이 약해진다. 결국 재판은 조애나가 승소한다. 조애나는 테드에게 변호사의 과격한 행동을 사과하지만 테드는 이미 돌아선 상태다.\n",
      "\n",
      "빌리가 조애나의 집으로 가는 날 부자는 이별의 포옹을 한다. 조애나가 집으로 찾아와 테드만 잠시 부른다. 조애나는 눈물을 흘리며, 빌리를 생각해서라도 데려가지 않는 것으로 결정했다고 이야기한다. 테드도 마찬가지로 눈물을 머금고 조애나와 포옹한다. 빌리를 보러 가겠다는 조애나에게 테드는 혼자 남아 있을 테니 아들과 둘이서만 보라고 한다. 조애나가 엘리베이터에 오르며 오늘 어떻게 보이냐고 묻고, 테드는 멋지다고 답한다.\n",
      "-----\n",
      "문서4: 이렇게 발견된 첫 번째 미라는 엑스레이 검사를 받았는데, \"기형 두개골 때문에 작은 어른처럼 보인\" 무뇌증 아기의 미라인 것으로 밝혀졌다. 1990년대 와이오밍대의 고고학자였던 조지 길과 덴버 어린이병원이 공동으로 조사한 두 번째 미라도 무뇌증에 걸린 아기인 것으로 드러났다. DNA 검사 결과 아기는 아메리카 원주민으로 밝혀졌고 탄소 연대는 1700년대로 측정됐다. \n",
      "\n",
      "1979년 7월 7일 <캐스퍼 스타트리뷴>에 실린 기사에 의하면 첫 번째 미라는 발견되자마자 날조된 것이다, 아기다, 전설상의 '소인'이다를 두고 논쟁이 벌어지기 시작했다고 한다. 미라는 와이오밍 주 미티츠의 한 동네 약국에 자리를 잡았고 이후 몇년간 명물로 인기를 끌다가 캐스퍼에 사는 사업가인 이반 T. 굿맨이 구입했다. 그런 다음에는 뉴욕의 사업가인 리오너드 워들러에게 넘어갔고, 그 뒤부터 현재의 행방은 아무도 모른다. 당시 캐스퍼 스타트리뷴에서는 진화론이 잘못됐음을 증명하기 위해 사라진 미라를 찾는 사람에게 10,000달러의 상금을 주겠다고 썼다\n",
      "-----\n",
      "문서5: ;헤쿠바\n",
      "녹스의 네크로멘서들의 어둠의 유산의 상속자인 헤쿠바는 망각의 오브를 이용하여 새로운 죽은자의 군대를 다시 움직이려는 무서운 음모를 가지고 있다.그녀는 죽은자의 땅의 폐허에 숨어서 오우거와 좀비, 수많은 언데드들로 구성된 그녀의 군대를 보내 녹스의 주민들을 공포에 떨게 하고 있는 장본인이다.\n",
      "\n",
      ";공중전함 함장 (잔도)\n",
      "세상의 관망자. 공중전함 함장은 많은 사람들에게 알려져 있지만 소수만이 그를 잘 안다고 말할 수 있다. 녹스의 여러곳을 떠돌면서 그는 녹스를 주의 깊게 관찰하고 있다.\n",
      "\n",
      ";소환술사 알드윈\n",
      "녹스의 마스터 소환술사이자 네크로맨서 전쟁의 영웅인 알드윈은 익스마을의 동쪽 자그마한 오두막에서 은거하며 살고 있다. 그는 대부분의 시간을 어떻게 새로운 비스트를 매료하고 소환할 것인지를 연구하는데 종사하고 있다.\n",
      "\n",
      ";대마법사 호바스\n",
      "예전의 호바스는 잔도가 네크로맨서 전쟁을 끝낼때 사용했던 아티팩트인 망각의 지팡이의 주요 제작자 중 한사람 이었다. 지금의 호바스는 갈라바 성에 살면서 마법 견습생들을 수련시키고 그곳에 사는 마법사들의 공동체에서 여러 가지 일거리를 돌보고 있으며 아직까지도 최고의 마법력을 가지고 있다.\n",
      "\n",
      ";워로드 호렌더스\n",
      "던 미르 요새의 파이어 나이츠의 사나운 감독인 워로드 호렌더스는 전설적인 전사들을 훈련시키고 있다. 필연적으로 잔도와 비교되는 것에 자극받아 호렌더스는 그의 이생을 그가 만나는 어떤 사람보다도 뛰어나다는 것을 증명하려고 노력하고 있다.\n",
      "\n",
      ";네크로멘서\n",
      "헤쿠바의 오른팔로서 흑마법을 쓰는 계열의 마법사이다. 그러나 자신의 영혼을 악에 팔고 그것으로 마법의 능력을 얻은 마법사이기 때문에 헤쿠바의 명령을 받아 게임상에서 주인공인 플레이어를 심심치 않게 괴롭히는 인물이다<|im_end|>\n",
      "<|im_start|>user\n",
      "콘코바르가 왕이 된 이후 이멘마하에서 있었던 첫 번째 전투는 무엇이었나?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "검색 결과에는 콘코바르가 왕이 된 이후 이멘마하에서 있었던 첫 번째 전투를 찾을 수 없습니다.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 템플릿 적용\n",
    "text = tokenizer.apply_chat_template(\n",
    "    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e1c7a7d-efde-4b0c-a2db-b337acd613b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a505ce87-8b94-44f9-a301-6e08f142f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"qwen2-7b-rag-ko\",        # 저장될 디렉터리와 저장소 ID\n",
    "    num_train_epochs=3,                  # 학습할 총 에포크 수 \n",
    "    per_device_train_batch_size=2,       # GPU당 배치 크기\n",
    "    gradient_accumulation_steps=2,       # 그래디언트 누적 스텝 수\n",
    "    gradient_checkpointing=True,         # 메모리 절약을 위한 체크포인팅\n",
    "    optim=\"adamw_torch_fused\",           # 최적화기\n",
    "    logging_steps=10,                    # 로그 기록 주기\n",
    "    save_strategy=\"steps\",               # 저장 전략\n",
    "    save_steps=50,                       # 저장 주기\n",
    "    bf16=True,                           # bfloat16 사용\n",
    "    learning_rate=1e-4,                  # 학습률\n",
    "    max_grad_norm=0.3,                   # 그래디언트 클리핑\n",
    "    warmup_ratio=0.03,                   # 워밍업 비율\n",
    "    lr_scheduler_type=\"constant\",        # 고정 학습률\n",
    "    push_to_hub=False,                   # 허브 업로드 안 함\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a15ea25d-fcf4-41db-9346-87f7ecbbad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    for example in batch:\n",
    "        # messages의 각 내용에서 개행문자 제거\n",
    "        clean_messages = []\n",
    "        for message in example[\"messages\"]:\n",
    "            clean_message = {\n",
    "                \"role\": message[\"role\"],\n",
    "                \"content\": message[\"content\"]\n",
    "            }\n",
    "            clean_messages.append(clean_message)\n",
    "        \n",
    "        # 깨끗해진 메시지로 템플릿 적용\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            clean_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        ).strip()\n",
    "        \n",
    "        # 텍스트를 토큰화\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        \n",
    "        # 레이블 초기화\n",
    "        labels = [-100] * len(input_ids)\n",
    "        \n",
    "        # assistant 응답 부분 찾기\n",
    "        im_start = \"<|im_start|>\"\n",
    "        im_end = \"<|im_end|>\"\n",
    "        assistant = \"assistant\"\n",
    "        \n",
    "        # 토큰 ID 가져오기\n",
    "        im_start_tokens = tokenizer.encode(im_start, add_special_tokens=False)\n",
    "        im_end_tokens = tokenizer.encode(im_end, add_special_tokens=False)\n",
    "        assistant_tokens = tokenizer.encode(assistant, add_special_tokens=False)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            # <|im_start|>assistant 찾기\n",
    "            if (i + len(im_start_tokens) <= len(input_ids) and \n",
    "                input_ids[i:i+len(im_start_tokens)] == im_start_tokens):\n",
    "                \n",
    "                # assistant 토큰 찾기\n",
    "                assistant_pos = i + len(im_start_tokens)\n",
    "                if (assistant_pos + len(assistant_tokens) <= len(input_ids) and \n",
    "                    input_ids[assistant_pos:assistant_pos+len(assistant_tokens)] == assistant_tokens):\n",
    "                    \n",
    "                    # assistant 응답의 시작 위치로 이동\n",
    "                    current_pos = assistant_pos + len(assistant_tokens)\n",
    "                    \n",
    "                    # <|im_end|>를 찾을 때까지 레이블 설정\n",
    "                    while current_pos < len(input_ids):\n",
    "                        if (current_pos + len(im_end_tokens) <= len(input_ids) and \n",
    "                            input_ids[current_pos:current_pos+len(im_end_tokens)] == im_end_tokens):\n",
    "                            # <|im_end|> 토큰도 레이블에 포함\n",
    "                            for j in range(len(im_end_tokens)):\n",
    "                                labels[current_pos + j] = input_ids[current_pos + j]\n",
    "                            break\n",
    "                        labels[current_pos] = input_ids[current_pos]\n",
    "                        current_pos += 1\n",
    "                    \n",
    "                    i = current_pos\n",
    "                \n",
    "            i += 1\n",
    "        \n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "    \n",
    "    # 패딩 적용\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n",
    "    \n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        padding_length = max_length - len(new_batch[\"input_ids\"][i])\n",
    "        \n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * padding_length)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * padding_length)\n",
    "        new_batch[\"labels\"][i].extend([-100] * padding_length)\n",
    "    \n",
    "    # 텐서로 변환\n",
    "    for k, v in new_batch.items():\n",
    "        new_batch[k] = torch.tensor(v)\n",
    "    \n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a68b49a-dff3-4883-a9ac-570b49cb747e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "[151644, 8948, 198, 64795, 82528, 33704, 85322, 77226, 98801, 18411, 81718, 144059, 42039, 138520, 19391, 143604, 129264, 130650, 382, 13146, 48431, 20401, 66790, 29326, 131193, 17877, 125686, 125548, 139713, 624, 16, 13, 138520, 53680, 85322, 77226, 98801, 18411, 81718, 144059, 42039, 143604, 16186, 139713, 624, 17, 13, 85322, 77226, 98801, 19391, 130768, 130213, 17877, 143604, 16186, 125476, 34395, 53900, 21329, 95577, 139713, 624, 18, 13, 138520, 19391, 128605, 143603, 12802, 85322, 77226, 98801, 19391, 130671, 32290, 85322, 77226, 98801, 126377, 330, 33883, 64795, 138520, 93, 19391, 128605, 130213, 12802, 136673, 1189, 129254, 143604, 16186, 139713, 624, 19, 13, 143604, 47836, 53618, 142976, 139236, 18411, 142616, 82190, 53435, 40853, 129549, 53435, 125068, 17877, 140174, 128836, 32290, 5140, 240, 97, 19391, 36330, 250, 125746, 16560, 23084, 126402, 83634, 17380, 94613, 139236, 84621, 47324, 18411, 129624, 20487, 139713, 13, 95617, 18411, 129901, 142976, 53435, 40853, 129835, 53435, 125068, 17877, 220, 16, 42044, 139236, 56475, 58677, 26699, 128836, 32290, 5140, 240, 97, 19391, 4318, 1097, 16, 5053, 130939, 54116, 57132, 16186, 139713, 624, 20, 13, 95617, 18411, 129901, 142976, 53435, 40853, 129835, 53435, 125068, 17877, 220, 16, 42044, 139236, 80573, 220, 20, 42044, 139236, 56475, 143409, 58677, 26699, 128836, 32290, 5140, 240, 97, 19391, 4318, 1097, 16, 20492, 4318, 1097, 20, 5053, 129254, 54116, 57132, 16186, 139713, 624, 21, 13, 81173, 66845, 23573, 49367, 23259, 20401, 139236, 18411, 58677, 26699, 82190, 143604, 16186, 139713, 382, 129845, 77226, 98801, 510, 34764, 51588, 26698, 16, 25, 3315, 121, 246, 129695, 130271, 125548, 16560, 136724, 3315, 96, 121, 57801, 97143, 126551, 11, 54825, 45130, 94, 126550, 33704, 131366, 129330, 13, 5140, 45881, 133507, 20401, 74884, 243, 51391, 24897, 98927, 48606, 60294, 20401, 5140, 229, 234, 18411, 124459, 111, 144190, 23894, 119, 17877, 65722, 116, 50340, 20401, 3315, 65291, 129378, 12802, 56419, 28002, 125678, 42039, 131670, 132236, 126551, 11, 3315, 65291, 60315, 81133, 125166, 20401, 56419, 55054, 3315, 120, 222, 28626, 32985, 231, 95577, 19969, 81133, 19969, 136342, 10764, 249, 242, 133847, 138863, 13146, 13, 128719, 3315, 120, 222, 28626, 16560, 125149, 144231, 129865, 17380, 51391, 24897, 98927, 48606, 60294, 20401, 5140, 229, 234, 18411, 126365, 246, 83277, 3315, 121, 246, 129695, 130271, 125548, 20401, 137767, 116, 133886, 131417, 132526, 125199, 34395, 11, 51391, 24897, 98927, 48606, 60294, 20401, 5140, 229, 234, 19969, 3315, 121, 246, 129695, 130271, 125548, 20401, 137767, 116, 28002, 125160, 130507, 232, 135363, 125511, 22042, 243, 128173, 86831, 134828, 13146, 13, 23084, 136168, 83556, 31079, 126588, 45130, 111, 33704, 124657, 125548, 129616, 125548, 12317, 81, 34196, 8, 20401, 95996, 60294, 48408, 144015, 5349, 64, 457, 19698, 701, 45130, 100, 133857, 129378, 20401, 85413, 101, 53189, 56039, 24897, 55673, 91043, 125548, 144111, 28002, 126445, 129254, 129112, 13, 3315, 121, 246, 129695, 130271, 125548, 20401, 141888, 128901, 23084, 23084, 126251, 128732, 17877, 62071, 92192, 47836, 28733, 46682, 125199, 34395, 11, 58034, 125746, 18411, 5140, 112, 231, 128747, 72553, 61298, 5140, 240, 97, 74884, 243, 126327, 10764, 251, 98, 79716, 87425, 50696, 89940, 141659, 17877, 139871, 47836, 28733, 137119, 126254, 128836, 13, 220, 22, 126216, 12802, 69441, 231, 56290, 139844, 57801, 10764, 251, 246, 60294, 62275, 5140, 240, 97, 3315, 121, 246, 129695, 130271, 125548, 16560, 142104, 19969, 3315, 96, 121, 125761, 16560, 126291, 76337, 17877, 55838, 96, 57801, 97143, 31079, 128618, 127121, 125580, 34395, 11, 5140, 229, 234, 19969, 10764, 226, 108, 83277, 3315, 96, 121, 125761, 13, 137767, 116, 28002, 19969, 10764, 226, 108, 85251, 140210, 56475, 5140, 123, 250, 31079, 83277, 60315, 130000, 142510, 20401, 125674, 131000, 18411, 83596, 33704, 98801, 54825, 16560, 54116, 129502, 124546, 135227, 97143, 125199, 34395, 54825, 126440, 132557, 33704, 48364, 250, 124785, 42039, 16778, 242, 13146, 13, 3315, 121, 246, 129695, 130271, 125548, 20401, 3315, 96, 121, 48431, 19391, 130207, 23084, 54116, 49664, 33704, 137016, 79302, 226, 144974, 23573, 54116, 129502, 124546, 56290, 19969, 139647, 83277, 134563, 11, 61298, 129027, 127042, 125548, 29346, 128753, 56290, 20401, 10764, 228, 254, 125548, 19969, 10764, 251, 238, 145873, 48606, 83036, 125548, 80573, 28927, 116, 40281, 13146, 137767, 116, 28002, 19391, 69192, 104, 133485, 12802, 22042, 243, 144190, 130861, 80573, 126310, 55054, 23573, 18585, 238, 12802, 90686, 13, 124685, 144255, 32290, 129419, 130861, 16560, 125703, 20401, 54116, 54321, 17877, 125466, 125522, 135405, 129549, 81718, 12802, 144120, 12802, 90486, 129807, 19391, 126440, 129321, 17877, 125714, 59698, 125615, 44518, 20487, 7, 127121, 125548, 29346, 12, 57801, 32077, 8, 19391, 28733, 43866, 52300, 71108, 32077, 134013, 90686, 382, 126893, 50340, 31328, 128901, 137289, 47985, 3315, 65291, 60315, 81133, 125166, 19391, 32985, 251, 79632, 70943, 12802, 125615, 3315, 121, 246, 129695, 130271, 125548, 20401, 48408, 64850, 3315, 65291, 125548, 127559, 3315, 121, 246, 29346, 70585, 109, 19969, 24897, 18411, 74884, 243, 42039, 57835, 66845, 66425, 130039, 83315, 144269, 140490, 11, 23084, 144178, 125544, 16186, 17380, 137844, 139097, 40771, 116, 19391, 3315, 65291, 125548, 127559, 33704, 54116, 52959, 24897, 18411, 124685, 20487, 57801, 97143, 125199, 34395, 49367, 3315, 65291, 129616, 5432, 64, 910, 16846, 8, 83518, 124780, 19391, 133552, 113, 125615, 70943, 19391, 79207, 113, 126614, 64795, 33883, 142510, 136999, 64795, 128836, 13, 23084, 19391, 3315, 65291, 129378, 3315, 120, 222, 125548, 60315, 81133, 19969, 3315, 121, 246, 129695, 130271, 125548, 20401, 128772, 48408, 64850, 3315, 123, 254, 24897, 50340, 29346, 48108, 246, 29346, 95577, 16186, 3025, 6654, 2388, 13831, 45111, 18294, 64, 8, 18411, 74884, 243, 42039, 57835, 66845, 128836, 624, 34764, 51588, 26698, 17, 25, 3315, 120, 222, 28626, 32985, 231, 95577, 19969, 81133, 3025, 295, 8978, 386, 27693, 610, 8, 16560, 48408, 32077, 136499, 29346, 128753, 56290, 20401, 138665, 24897, 33861, 60960, 124781, 19391, 77002, 40853, 42905, 58677, 126251, 125489, 13, 3315, 65291, 60315, 81133, 125166, 20401, 56419, 55054, 17380, 11, 65722, 116, 50340, 20401, 3315, 65291, 129378, 3315, 120, 222, 125548, 60315, 81133, 20401, 135968, 23259, 125489, 382, 31079, 144661, 56419, 130766, 64850, 129889, 3315, 120, 222, 28626, 19969, 3315, 65291, 129378, 20401, 124685, 132125, 83036, 20136, 222, 29346, 129695, 144557, 53680, 129624, 129865, 21329, 62275, 11, 3315, 65291, 129378, 53680, 16560, 69192, 247, 128732, 62275, 130939, 47985, 129112, 382, 50340, 133507, 124889, 32985, 231, 49367, 91043, 20401, 130263, 56475, 77353, 61741, 19969, 47665, 234, 31079, 128036, 17877, 53618, 11, 3315, 65291, 60315, 81133, 125166, 80573, 65722, 116, 50340, 20401, 127296, 56419, 55054, 126253, 3315, 123, 254, 50340, 29346, 56039, 125548, 18411, 5140, 228, 241, 34395, 126804, 131958, 24485, 227, 80968, 17877, 64577, 133738, 130705, 34143, 44680, 230, 105, 125761, 13, 3315, 120, 222, 28626, 16560, 56419, 55054, 64850, 126804, 126317, 17877, 127218, 12802, 130108, 23084, 144141, 16560, 21329, 129900, 133339, 136262, 54825, 64850, 130593, 20401, 38150, 17877, 49367, 126251, 57801, 127579, 125761, 13, 3315, 120, 222, 28626, 19969, 3315, 123, 254, 50340, 29346, 56039, 125548, 18411, 88846, 19969, 125476, 16560, 143803, 3315, 65291, 129378, 3315, 120, 222, 125548, 60315, 81133, 19969, 129392, 135375, 128836, 13, 3315, 65291, 129378, 33704, 127218, 12802, 3315, 120, 222, 28626, 129885, 124657, 80901, 19391, 137119, 140200, 125580, 34395, 11, 3315, 120, 222, 28626, 16560, 45104, 101, 130609, 18411, 139180, 128836, 13, 130549, 134748, 126322, 76435, 48408, 127478, 144457, 12802, 137046, 32290, 54825, 19969, 3315, 65291, 129378, 20401, 124657, 80901, 19391, 132236, 17877, 71108, 130939, 140200, 128836, 13, 125261, 25715, 3315, 65291, 129378, 33704, 74808, 125052, 126720, 50340, 130000, 48408, 127478, 144457, 20401, 137767, 116, 133886, 3315, 120, 222, 28626, 126327, 126365, 246, 83277, 134794, 132091, 16751, 239, 23259, 128836, 13, 4710, 136133, 28626, 16560, 65722, 116, 50340, 20401, 74884, 243, 3315, 121, 246, 129695, 130271, 125548, 32985, 231, 65553, 97, 134445, 3315, 96, 121, 124982, 126551, 11, 54825, 130861, 16560, 131366, 129330, 13, 3315, 65291, 129378, 12802, 5140, 45881, 133507, 20401, 74884, 243, 51391, 24897, 98927, 48606, 60294, 18411, 3315, 96, 121, 133215, 54825, 5140, 229, 234, 18411, 124459, 111, 144190, 129337, 127819, 116, 17380, 129262, 134088, 52959, 129882, 34395, 49367, 144723, 126551, 11, 3315, 120, 222, 28626, 19969, 139484, 10764, 249, 242, 133847, 135448, 125149, 144231, 129865, 17380, 126365, 246, 83277, 3315, 121, 246, 129695, 130271, 125548, 20401, 137767, 116, 28002, 125160, 19391, 54969, 144158, 125761, 13, 3315, 121, 246, 129695, 130271, 125548, 20401, 141888, 128901, 74884, 243, 17877, 3315, 96, 121, 12802, 21329, 127728, 130408, 23084, 23084, 126251, 128732, 17877, 5140, 117, 120, 144038, 130864, 17877, 138037, 21329, 129293, 128836, 13, 129548, 60960, 131347, 5140, 112, 231, 128747, 23573, 5140, 240, 97, 74884, 243, 126327, 66790, 60315, 59698, 57801, 10764, 251, 98, 79716, 87425, 127728, 133099, 134952, 144024, 17877, 85403, 21329, 47836, 28733, 137119, 126254, 128836, 13, 220, 22, 126216, 12802, 69441, 231, 56290, 139844, 57801, 10764, 251, 246, 60294, 62275, 5140, 223, 251, 19391, 3315, 121, 246, 129695, 130271, 125548, 16560, 142104, 19969, 3315, 96, 121, 125761, 16560, 126291, 76337, 17877, 55838, 96, 57801, 97143, 125761, 13, 54825, 16560, 128618, 127121, 125580, 34395, 11, 54825, 81718, 133016, 19391, 5140, 229, 234, 19969, 10764, 226, 108, 83277, 3315, 96, 121, 125761, 382, 31079, 126039, 23894, 101, 126893, 129378, 3315, 120, 222, 28626, 19969, 65722, 116, 50340, 17380, 79207, 113, 126614, 17877, 73518, 19969, 65722, 116, 50340, 129624, 25715, 141767, 126251, 32077, 144238, 130345, 17877, 3315, 96, 121, 133215, 54825, 28733, 128911, 129125, 47665, 254, 31079, 138863, 13146, 13, 138658, 230, 12802, 66136, 134828, 132264, 3315, 65291, 129378, 33704, 3315, 120, 222, 28626, 20401, 10764, 251, 242, 80968, 17877, 57835, 80968, 47836, 28733, 137046, 13, 3315, 65291, 129378, 33704, 3315, 120, 222, 28626, 18411, 126629, 132499, 126002, 125590, 54825, 18411, 60960, 80968, 66425, 55673, 126781, 140490, 11, 56419, 125625, 18411, 36978, 108, 125615, 95577, 63089, 19969, 54825, 18411, 141693, 131833, 125866, 73986, 126588, 140026, 131766, 17877, 126429, 133847, 133339, 34395, 73518, 26698, 57801, 97143, 125761, 13, 5140, 239, 62618, 16560, 61298, 83518, 126893, 56475, 83556, 60960, 83556, 82619, 129924, 18411, 47665, 234, 124982, 34395, 11, 23894, 102, 144210, 23573, 28927, 116, 133914, 5140, 223, 251, 19391, 3315, 65291, 129378, 12802, 3315, 120, 222, 28626, 18411, 3315, 96, 121, 124982, 127378, 3315, 65291, 129378, 129238, 31328, 47985, 5140, 117, 230, 55054, 55902, 86372, 19391, 5140, 117, 254, 141798, 624, 34764, 51588, 26698, 18, 25, 5140, 231, 112, 138081, 20401, 8620, 112, 239, 34395, 98005, 55054, 56475, 83556, 42905, 10764, 72509, 29346, 143230, 135152, 132125, 16560, 143861, 98, 28754, 64521, 83556, 144366, 12802, 125590, 85413, 234, 131973, 144117, 144041, 12802, 136108, 35509, 29281, 126377, 126291, 144117, 23573, 130887, 125489, 13, 48408, 95218, 65510, 126898, 60315, 16560, 10764, 72509, 29346, 20401, 134312, 222, 19969, 19969, 143861, 99, 33704, 133487, 37195, 254, 16778, 239, 67511, 24897, 134798, 57801, 132819, 18411, 22042, 251, 125511, 21329, 135201, 23084, 132557, 17877, 125206, 41671, 23573, 5140, 240, 97, 130263, 17877, 73518, 19969, 86831, 129807, 13146, 13, 10764, 72509, 29346, 16560, 23872, 254, 135660, 61298, 130237, 62275, 20401, 46319, 138789, 133215, 40771, 230, 126321, 137844, 57268, 124923, 21329, 128956, 128646, 33704, 130331, 11, 45130, 100, 48408, 64850, 5140, 117, 234, 133886, 137843, 41671, 131611, 89860, 144959, 130263, 126246, 32077, 17877, 60716, 89659, 95002, 71647, 21329, 19969, 130723, 13, 83556, 53680, 79302, 239, 126596, 17877, 30520, 239, 124528, 16186, 126039, 50340, 98005, 55054, 56475, 20401, 69441, 231, 129382, 33704, 73518, 134246, 134497, 11, 5140, 117, 234, 132920, 133788, 60716, 26698, 23084, 31079, 130974, 24485, 226, 125544, 20401, 85403, 57132, 129147, 47911, 250, 128844, 17877, 37195, 116, 13146, 13, 34143, 44680, 51275, 125511, 47985, 130217, 12802, 66790, 60315, 131611, 85403, 132343, 5140, 239, 246, 72553, 20401, 47818, 125991, 19391, 135968, 131518, 33883, 73518, 62275, 13146, 382, 130229, 29346, 16560, 48408, 144151, 126886, 19391, 32129, 16560, 23084, 138873, 12802, 25715, 65510, 126898, 60315, 80573, 90711, 250, 134965, 95577, 92192, 144231, 53680, 90711, 250, 33883, 85251, 13146, 13, 95577, 92192, 144231, 47985, 10764, 72509, 29346, 80573, 95577, 138143, 19969, 21329, 17380, 23084, 132557, 94315, 64577, 76337, 64850, 47455, 120, 25715, 10764, 92120, 40281, 34395, 132236, 33509, 137621, 5140, 239, 246, 33704, 137765, 20401, 71647, 21329, 19391, 125466, 129567, 134965, 128900, 13, 53900, 126746, 16560, 16235, 254, 29326, 61298, 134182, 17877, 45104, 242, 125615, 136541, 5140, 117, 234, 28002, 19969, 36055, 83291, 144042, 56475, 57835, 132772, 33883, 137767, 116, 28002, 19969, 62099, 95, 31079, 130974, 138629, 12802, 47665, 234, 31079, 85251, 13146, 13, 45710, 128911, 23573, 10764, 72509, 29346, 16560, 48408, 129125, 95170, 34395, 137768, 126352, 133886, 34143, 105, 125476, 30520, 239, 54321, 19391, 5140, 41902, 125476, 19969, 34395, 11, 85403, 32831, 126898, 18411, 95996, 144047, 33883, 48408, 129360, 28733, 125880, 17877, 66790, 136387, 41671, 124905, 48408, 129125, 95170, 125512, 136133, 129112, 13, 28733, 125880, 94315, 10764, 72509, 29346, 16560, 95577, 92192, 144231, 17877, 5140, 117, 234, 28002, 20401, 60960, 129439, 17380, 127165, 120, 16560, 13146, 382, 92817, 126898, 60315, 19969, 5140, 244, 254, 126588, 66790, 220, 16, 20, 59761, 128514, 62107, 19391, 5140, 231, 112, 138081, 19391, 137844, 80573, 10764, 72509, 29346, 80573, 62107, 126588, 13146, 13, 5140, 239, 246, 33704, 136065, 126377, 90711, 250, 134372, 128836, 13, 65510, 126898, 60315, 16560, 131823, 19969, 5140, 244, 254, 126588, 129274, 48408, 129125, 10764, 92120, 126893, 138268, 18411, 53900, 20487, 133083, 124982, 127378, 11, 132911, 48408, 129360, 79302, 239, 126596, 128739, 17877, 34143, 105, 129254, 129112, 13, 54825, 126254, 17877, 129900, 33704, 10764, 72509, 29346, 16560, 143231, 46832, 242, 18411, 66136, 124905, 64577, 133886, 22042, 243, 125625, 34395, 73518, 125519, 13146, 13, 46319, 47324, 55054, 19969, 126429, 26699, 130357, 45130, 100, 126785, 29281, 34143, 44680, 230, 120, 42039, 95996, 65865, 51876, 13, 126488, 124528, 125511, 47985, 129242, 129382, 17877, 139274, 126923, 34395, 10764, 72509, 29346, 19969, 137351, 40853, 56475, 60716, 34395, 128841, 81718, 133016, 19391, 79302, 239, 126596, 128739, 129242, 129382, 19391, 126488, 28002, 128555, 46319, 140026, 11, 10764, 72509, 29346, 16560, 77353, 135402, 17877, 143231, 37195, 106, 144162, 19969, 131611, 128878, 134583, 137351, 137471, 58777, 51876, 382, 92817, 126898, 60315, 16560, 129242, 129382, 56475, 82619, 132557, 47818, 125991, 33704, 129093, 72553, 87425, 133995, 34395, 10764, 72509, 29346, 20401, 83556, 70943, 129502, 53680, 10764, 236, 116, 126110, 129147, 127218, 33704, 45104, 101, 92031, 89235, 25715, 12802, 127085, 135979, 20401, 8620, 123, 230, 17877, 143188, 137571, 10764, 244, 19946, 34395, 132376, 251, 129709, 51876, 13, 129082, 33704, 137351, 124517, 17877, 79302, 119, 31079, 10764, 72509, 29346, 129885, 47985, 126366, 77353, 135402, 12802, 5140, 228, 240, 34395, 35509, 29281, 19391, 126291, 144117, 134965, 10764, 72509, 29346, 129885, 127218, 12802, 5140, 117, 234, 133886, 10764, 92120, 40281, 132264, 126366, 135968, 128747, 129330, 34395, 126254, 51876, 13, 10764, 72509, 29346, 16560, 65510, 126898, 60315, 20401, 126429, 130728, 19391, 131766, 12802, 85413, 222, 125545, 12802, 125590, 11, 46319, 47324, 55054, 19969, 82619, 132557, 47818, 125991, 20401, 54969, 135356, 129093, 31328, 33704, 65510, 126898, 60315, 129254, 36978, 108, 52959, 141142, 57026, 129242, 129382, 33704, 90711, 246, 53955, 33883, 85251, 13146, 13, 129419, 84621, 83666, 129242, 129382, 56475, 11, 95577, 92192, 144231, 12802, 132376, 251, 31328, 42039, 73518, 80573, 10764, 72509, 29346, 19969, 46319, 128836, 34395, 38523, 117, 47324, 87425, 72553, 11, 65510, 126898, 60315, 20401, 46319, 47324, 55054, 19969, 65510, 126898, 60315, 18411, 5140, 244, 254, 60315, 126054, 126310, 47985, 23573, 129274, 130737, 95577, 92192, 144231, 12802, 125199, 138349, 22042, 251, 125511, 124905, 57852, 132524, 28754, 12802, 23872, 121, 33883, 85251, 13146, 13, 136724, 129242, 129382, 33704, 65510, 126898, 60315, 19969, 79207, 117, 43590, 51876, 13, 65510, 126898, 60315, 16560, 10764, 72509, 29346, 126327, 46319, 47324, 55054, 20401, 45130, 120, 126614, 23573, 140692, 17877, 32129, 53680, 87425, 72553, 10764, 72509, 29346, 16560, 90667, 137844, 125519, 137619, 13146, 382, 143952, 28002, 19969, 65510, 126898, 60315, 20401, 130263, 42039, 35509, 16560, 37195, 254, 85403, 132343, 23084, 126591, 20401, 98869, 144547, 17877, 129112, 13, 65510, 126898, 60315, 19969, 130263, 42039, 138038, 80573, 10764, 72509, 29346, 72553, 16235, 254, 29326, 85403, 82965, 13146, 13, 65510, 126898, 60315, 16560, 138658, 230, 137075, 10764, 251, 246, 28002, 124905, 11, 5140, 117, 234, 133886, 128646, 96137, 127937, 5140, 41902, 125476, 19969, 21329, 127728, 132091, 135763, 128836, 34395, 130861, 51876, 13, 10764, 72509, 29346, 47985, 95577, 138143, 19969, 21329, 17380, 138658, 230, 137075, 137767, 116, 125052, 34395, 65510, 126898, 60315, 80573, 98869, 144547, 51876, 13, 5140, 117, 234, 133886, 63332, 60294, 35509, 127816, 16560, 65510, 126898, 60315, 126327, 10764, 72509, 29346, 16560, 47455, 120, 25715, 129624, 52959, 130689, 10764, 72509, 83036, 48408, 134771, 5140, 239, 62618, 26698, 72553, 63332, 129254, 129112, 13, 65510, 126898, 60315, 19969, 24485, 246, 28002, 132947, 12802, 33861, 19391, 73077, 125548, 124905, 133857, 130108, 63332, 12802, 127673, 34395, 133552, 119, 34395, 11, 10764, 72509, 29346, 16560, 48108, 233, 21329, 131042, 143603, 51876, 624, 34764, 51588, 26698, 19, 25, 130653, 142300, 52300, 48364, 104, 84621, 83666, 125714, 129615, 24485, 239, 24897, 135152, 85322, 134445, 83596, 126002, 126551, 11, 330, 20487, 128909, 129419, 59761, 138924, 129147, 143416, 124685, 82965, 131137, 63332, 31328, 1, 125149, 144092, 128844, 48408, 20487, 20401, 125714, 50340, 31328, 132091, 22042, 251, 128173, 141798, 13, 220, 16, 24, 24, 15, 126216, 66845, 74884, 222, 12802, 57268, 144276, 66845, 20401, 126429, 34395, 124632, 25715, 124982, 125615, 65510, 21329, 40771, 116, 53680, 126365, 112, 79004, 124685, 129807, 12802, 142197, 12802, 140429, 42039, 65510, 55054, 23573, 129419, 84621, 83666, 125714, 127937, 125149, 144092, 128844, 19391, 131961, 129807, 48408, 20487, 31328, 132091, 55838, 250, 60294, 138097, 13146, 13, 15552, 85322, 55054, 98801, 48408, 135854, 48408, 84667, 28002, 129616, 129093, 138533, 42039, 22042, 251, 128173, 128036, 34395, 74361, 226, 43590, 77353, 66845, 16560, 220, 16, 22, 15, 15, 126216, 129923, 3315, 116, 94, 29281, 134521, 13146, 13, 4710, 16, 24, 22, 24, 126216, 220, 22, 128514, 220, 22, 32077, 366, 142592, 24897, 136275, 79207, 44680, 225, 222, 28626, 28002, 147452, 29, 19391, 126423, 129807, 54116, 55054, 19391, 124970, 126559, 48364, 104, 84621, 83666, 125714, 129615, 142300, 64119, 25715, 125544, 25715, 37195, 254, 92817, 52300, 128900, 11, 48408, 20487, 13146, 11, 56419, 125624, 55902, 20401, 364, 43590, 31328, 6, 125489, 18411, 129419, 34395, 127041, 120, 131833, 12802, 47665, 234, 31079, 21329, 20487, 93721, 128836, 34395, 129112, 13, 125714, 129615, 74884, 222, 12802, 57268, 144276, 55673, 125714, 131131, 142852, 20401, 61298, 126322, 126730, 23872, 121, 124785, 19391, 64577, 133886, 16235, 94, 126002, 34395, 136331, 36978, 229, 126216, 62275, 130345, 126251, 17380, 58677, 131777, 5140, 223, 234, 135448, 3315, 118, 238, 24897, 136275, 19391, 32129, 16560, 131608, 19969, 31328, 23084, 126641, 350, 13, 124459, 123, 144143, 12802, 58777, 43866, 128836, 13, 126615, 126844, 126377, 5140, 231, 112, 138081, 20401, 131608, 19969, 31328, 56983, 57268, 127085, 29346, 85413, 234, 64850, 60294, 126327, 143836, 138863, 34395, 11, 54825, 5140, 240, 97, 126558, 132270, 20401, 126174, 126321, 33704, 134006, 47985, 54070, 82965, 13146, 13, 140730, 3315, 118, 238, 24897, 136275, 79207, 44680, 225, 222, 28626, 28002, 147452, 129889, 126616, 56290, 126605, 12802, 142357, 134521, 138349, 132376, 251, 79632, 66425, 130039, 32129, 50340, 85251, 125714, 50340, 18411, 138037, 16560, 124889, 126327, 220, 16, 15, 11, 15, 15, 15, 129062, 60294, 20401, 58034, 125052, 17877, 55673, 127816, 34395, 3315, 235, 120, 13146, 198, 34764, 51588, 26698, 20, 25, 2587, 142777, 141571, 130271, 198, 144067, 24897, 20401, 65553, 44680, 223, 105, 17380, 144178, 26698, 129360, 124685, 144719, 20401, 126310, 85057, 20401, 58034, 126299, 25715, 31328, 10764, 245, 97, 141571, 130271, 16560, 32985, 251, 126317, 20401, 73077, 131196, 18411, 126563, 82190, 134585, 3315, 96, 121, 33704, 131958, 124459, 108, 66845, 18411, 131170, 85413, 222, 125545, 12802, 125476, 16560, 125149, 26698, 93672, 16751, 234, 129439, 18411, 131670, 90686, 13, 48606, 132751, 16560, 3315, 96, 121, 33704, 131958, 5140, 243, 227, 20401, 69441, 238, 131976, 19391, 69192, 101, 136108, 73077, 40281, 92192, 80573, 130572, 70582, 11, 28733, 131456, 33704, 139957, 124419, 29346, 64850, 17380, 136239, 52300, 54825, 132751, 20401, 124459, 108, 66845, 18411, 141836, 127041, 117, 24897, 20401, 55673, 125496, 129125, 125466, 128808, 19391, 5140, 244, 101, 57801, 130127, 64521, 129359, 125822, 31328, 125489, 382, 26, 78125, 126402, 65865, 77953, 50972, 40853, 320, 144012, 47985, 340, 41429, 55902, 20401, 92751, 130472, 25715, 13, 125466, 126402, 65865, 77953, 50972, 40853, 33704, 129875, 131508, 126327, 138630, 83277, 141258, 126291, 23259, 72553, 12802, 54825, 18411, 126720, 23872, 19946, 34395, 126254, 47836, 28733, 90686, 13, 127041, 117, 24897, 20401, 127296, 130638, 17877, 5140, 244, 254, 133485, 131611, 54825, 16560, 127041, 117, 24897, 18411, 55673, 20401, 130507, 232, 57801, 92751, 132297, 126204, 90686, 382, 26, 43590, 65238, 125880, 55054, 125214, 29346, 144214, 198, 144067, 24897, 20401, 95577, 24897, 33861, 126291, 65238, 125880, 133414, 25715, 65553, 44680, 223, 105, 17380, 144143, 26698, 56419, 131833, 20401, 126440, 144308, 31328, 125214, 29346, 144214, 33704, 16751, 113, 24897, 125544, 17877, 20401, 126322, 132064, 64577, 48606, 125544, 23573, 73077, 126923, 127559, 56475, 16751, 222, 92192, 130705, 127166, 34395, 90686, 13, 54825, 16560, 140094, 20401, 130217, 17877, 130108, 134585, 73986, 53189, 18411, 126932, 63256, 126204, 126291, 65238, 47836, 71108, 31328, 133120, 131698, 134107, 98358, 55054, 126204, 90686, 382, 26, 66845, 125544, 126270, 55054, 91043, 130271, 24897, 198, 127027, 65865, 20401, 91043, 130271, 24897, 16560, 16235, 242, 47985, 19969, 65553, 44680, 223, 105, 17380, 144143, 26698, 56419, 131833, 17877, 5140, 223, 251, 144038, 131866, 40720, 134965, 48408, 131131, 144375, 28626, 31328, 32985, 251, 126317, 20401, 66790, 144789, 12802, 20401, 55673, 35711, 62071, 67511, 25715, 70943, 61298, 55054, 133016, 23084, 125761, 13, 129082, 20401, 91043, 130271, 24897, 16560, 16778, 230, 50340, 130271, 128677, 19391, 127166, 131611, 95577, 126270, 23894, 105, 35530, 76435, 129125, 28733, 125747, 135298, 34395, 54825, 130638, 19391, 32129, 16560, 95577, 126270, 55054, 129360, 140429, 49543, 56475, 127296, 127154, 83556, 92192, 133886, 137843, 130626, 132931, 137289, 128878, 47985, 81173, 34395, 20401, 95577, 126270, 135818, 131670, 90686, 382, 26, 130109, 17380, 29346, 91043, 144062, 125932, 24897, 198, 125615, 125714, 125548, 85997, 131097, 20401, 54969, 12802, 31079, 37195, 62618, 142852, 20401, 32129, 60315, 93672, 129423, 129502, 31328, 85413, 234, 17380, 29346, 91043, 144062, 125932, 24897, 16560, 56419, 125624, 128533, 56419, 55054, 129125, 10764, 249, 230, 125747, 135298, 34395, 90686, 13, 75528, 125568, 128552, 16235, 242, 47985, 80573, 142350, 128841, 71108, 19391, 64577, 131529, 132872, 52959, 91043, 144062, 125932, 24897, 16560, 54825, 20401, 23084, 76435, 17877, 54825, 19969, 142353, 16560, 129273, 124889, 129885, 47985, 5140, 249, 108, 31079, 60315, 130822, 129337, 132376, 251, 79632, 16186, 125476, 34395, 136111, 126204, 90686, 382, 26, 75142, 44680, 223, 105, 17380, 144178, 26698, 198, 142777, 141571, 130271, 20401, 73077, 82965, 143177, 135979, 10764, 251, 239, 125544, 126270, 17877, 3315, 241, 108, 16560, 94203, 53955, 20401, 95577, 126270, 55054, 125489, 13, 130549, 134748, 126440, 132557, 17877, 23872, 227, 19391, 45104, 242, 34395, 126038, 42039, 95577, 126270, 20401, 143861, 98, 135818, 79302, 119, 33704, 95577, 126270, 133414, 20487, 129147, 10764, 245, 97, 141571, 130271, 20401, 130345, 126691, 17877, 135839, 98927, 93701, 55902, 56475, 55673, 31328, 78125, 31328, 142509, 234, 135152, 31079, 18411, 28927, 105, 125512, 59698, 50696, 57801, 8620, 112, 112, 139844, 125511, 16560, 58677, 126251, 125489, 151645, 198, 151644, 872, 198, 139726, 129695, 130271, 125548, 19969, 74884, 243, 12802, 130722, 136331, 23084, 144178, 125544, 16186, 56475, 132236, 125615, 48364, 104, 84621, 83666, 56419, 129924, 16560, 132779, 12802, 125199, 60315, 30, 151645, 198, 151644, 77091, 198, 129845, 77226, 98801, 126377, 3315, 121, 246, 129695, 130271, 125548, 19969, 74884, 243, 12802, 130722, 136331, 23084, 144178, 125544, 16186, 56475, 132236, 125615, 48364, 104, 84621, 83666, 56419, 129924, 18411, 138037, 17877, 28733, 136673, 13, 151645]\n",
      "레이블에 대한 정수 인코딩 결과:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 129845, 77226, 98801, 126377, 3315, 121, 246, 129695, 130271, 125548, 19969, 74884, 243, 12802, 130722, 136331, 23084, 144178, 125544, 16186, 56475, 132236, 125615, 48364, 104, 84621, 83666, 56419, 129924, 18411, 138037, 17877, 28733, 136673, 13, 151645]\n"
     ]
    }
   ],
   "source": [
    "# 데이터의 최대 길이 한도를 지정. 최대 8192개의 토큰까지만 사용한다.\n",
    "max_seq_length=8192\n",
    "\n",
    "example = train_dataset[0]\n",
    "batch = collate_fn([example])\n",
    "\n",
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0].tolist())\n",
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd9ce18b-2539-4314-94d5-10464fb6586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [285/285 31:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.553900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.494500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.564800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.456700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.407200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.395800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.389600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.430200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.320900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.334400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.438700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.384500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.355100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.448800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.384400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.362900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.313400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.318900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.274300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.396100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이 설정\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.train()  # 모델이 자동으로 허브와 output_dir에 저장됨\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model()  # 최종 모델을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e7e2ca8-98e1-413a-a72c-100e394386e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for prompt in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        prompt, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    input = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant'\n",
    "    label = text.split('<|im_start|>assistant')[1]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b7e5668-0df5-4df5-96e1-074200456469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
      "\n",
      "다음의 지시사항을 따르십시오.\n",
      "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
      "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
      "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\"라고 답변하십시오.\n",
      "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
      "5. 예를 들어 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]라고 기재하십시오.\n",
      "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
      "\n",
      "검색 결과:\n",
      "-----\n",
      "문서1: ‘가치’를 통해 분석한 경제 현상◇밈노믹스=‘밈(meme)’은 유전을 통하지 않고 사회 속에서 세대 간 옮겨지는 문화 요소를 뜻하는 개념이다. 저자는 밈을 경제학에 접목해 ‘시장’ 원리가 아닌 인간이 추구하는 ‘가치’의 프리즘을 통해 경제 현상을 분석하고 미래 경제를 예측한다. 혼돈과 질서의 소용돌이 속에서 변화하고 발전하는 경제 흐름을 인류의 문화적 진화라는 독특한 관점에서 해석했다. (사이드 돌리바니 지음, 박세연 옮김, 엘도라도, 444쪽, 1만8000원)唐詩 찾아 중국 삼만리◇중국, 당시의 나라=김준연 고려대 중어중문학과 교수가 이백 두보 백거이 왕유 등이 지은 당시(唐詩)의 흥취를 느끼기 위해 당나라 시대 지도를 들고 길을 나섰다. 13개 성(省)에 산재한 수십 개의 시와 현을 찾아다니며 당시 200여수의 내력을 훑었다. 10여년간 1만2500㎞를 누빈 대장정의 기록이다. 답사한 명승고적에서 직접 확인할 수 있는 시를 우선적으로 소개했다. (김준연 지음, 궁리, 652쪽, 2만8000원)클래식~팝 … 음악의 본질◇리슨 투 디스=미국 음악 비평계를 선도적으로 이끌고 있는 저자가 클래식과 팝의 경계를 넘나들며 작곡가, 지휘자, 피아니스트, 록밴드, 싱어송라이터 등 다양한 음악가들의 자취를 따라가며 음악의 역사와 본질에 대해 이야기한다. 음악이 인간의 복잡다단한 면을 어떻게 담아내고, 어떤 식으로 살아남아 그 영역을 확장해 나가는지를 흥미롭게 풀어냈다. (알렉스 로스 지음, 장호연 옮김, 뮤진트리, 524쪽, 2만5000원)세월을 요리하는 식당◇백년식당=에세이 작가이자 요리사인 저자가 한국에서 50년 이상 영업한 오래된 식당들의 철학과 삶, 추억이 깃든 음식을 소개한다. 주인들의 기억을 끄집어내고 옛 문헌들을 찾아 ‘그 집’만의 특별함을 기록했다. 청진옥 할매국밥 우래옥 등 저자가 찾은 전국 18곳 식당의 공통점은 음식이 맛있고, 주인이 직접 일하고, 점원들의 근속 기간이 길다는 점이다. (박찬일 글, 노중훈 사진, 중앙M&B, 344쪽, 1만4800원)그들 눈에 비친 넬슨 만델라\n",
      "-----\n",
      "문서2: 대륙 내 돈의 흐름이 바뀌고 있다◇중국경제 다시 읽어라=KOTRA 상하이무역관에서 일하는 저자의 중국 경제 해설서. 중앙정부와 지방정부의 관계, 투자 위주의 성장 방식, 내수시장의 소비 성향, 서구화의 의미와 국제화를 대하는 중국 기업의 태도 등을 객관적으로 분석했다. 민주화 등 민감한 문제부터 금융과 관련한 본질적 문제, 은행과 주식 및 채권의 미래 등을 다각도로 제시한다. 위안화의 약점과 강점, 향후 전망 등도 살핀다.(김명신 지음, 북로드, 280쪽, 1만4000원)리커창 통해 바라본 중국의 미래◇리커창=중국 경제의 조타수라 불리는 리커창의 과거 행적을 들여다보고 현재 행보를 다각도로 살펴 중국의 미래를 예측한 책. 연대기적 과거를 나열하는 정치인 평전들과 달리 객관적인 시각과 깊은 분석을 담았다. 리커창의 오랜 친구이자 베이징대 동문인 구천서 한반도미래재단 이사장의 편역으로 깊이를 더했다. 리커창에 대한 객관적인 평가로 정작 중국에서는 판매가 금지된 책이다.(홍칭 지음, 구천서 편역, 푸른역사, 436쪽, 2만원)내 몸의 독 … 식품첨가물의 진실◇죽음을 부르는 맛의 유혹=MSG로 대표되는 식품첨가물이 인체에 유해하다는 주장을 담은 책. 논쟁이 계속되고 있지만 저자는 식품첨가물이 어린이의 학습장애와 행동장애, 고령층의 파킨슨병, 루게릭병 등 신경질환을 일으키는 데 영향을 준다고 말한다. MSG 등의 첨가물이 성장기의 뇌 발달 방식을 바꿔버릴 수도 있다는 것을 밝혀낸 실험, 신경계 질환과 관련한 중요한 연구 사례 등도 소개한다.(러셀 블레이록 지음, 강민재 옮김, 에코리브르, 448쪽, 2만원)엄마가 알려주는 화학 이야기\n",
      "-----\n",
      "문서3: 경제·경영●직업의 이동 첨단 기술과 초고령 사회가 바꾸고 있는 직업 세계를 조명했다. 향후 10~20년 안에 주목받게 될 직업군에 대해 살펴본다. 저자는 “미래 직업도 진입장벽과 전문성에 따라 소득이 결정될 것”이라고 강조한다.(신상진 지음, 한스미디어, 308쪽, 1만5000원) ●비난 게임 직장을 ‘잡아먹지 않으면 잡아먹히는 정글’로 인식해 일어나는 구성원 사이의 비난 행위를 지적한다. 이런 비난 게임이 어떻게 개인과 조직을 망가뜨리는지 보여준다.(벤 대트너·대런 달 지음, 홍경탁 옮김, 북카라반, 260쪽, 1만4000원) ●중국인은 어떻게 부를 축적하는가 중국인을 부자가 되려는 염원과 열망을 품고 자신의 생업과 교역 활동 의지가 충만한 사람들로 정의하고, 고대부터 최근까지 중국 부호들의 이야기를 담았다.(소준섭 지음, 한길사, 372쪽, 1만8000원)인문·교양 ●정의를 부탁해 25년 경력 베테랑 기자의 칼럼집. 한국 사회를 가로막고 있는 세대와 이념 갈등, 지역 장벽의 진실을 직시하고 새로운 지향점을 고민한다.(권석천 지음, 동아시아, 416쪽, 1만5000원) ●세상을 바꿔라 3 정치·경제 개혁, 국토 균형발전, 공기업 개혁, 국가재난 대응시스템 등을 주제로 현장 경험이 풍부한 저자들이 미래 비전을 제시한다.(함승희 외 지음, 오래, 310쪽, 2만원) ●어떤 아이들의 전생 기억에 관하여 전생을 기억한다는 세계 어린이 2500명을 탐구해 아이들의 말에 어떤 공통점이 있는지, 이것이 신뢰할 만한 주장인지 등을 깊이 있게 분석한다.(짐 터커 지음, 박인수 옮김, 김영사, 336쪽, 1만3000원)\n",
      "-----\n",
      "문서4: ◇새로운 디지털 시대=세계적 정보기술(IT) 기업 구글의 에릭 슈밋 회장과 구글의 싱크탱크인 구글 아이디어의 제러드 코언 소장이 쓴 IT 시대 미래 예측서. 세계인이 온라인 세상에서 만나게 될 모습을 다뤘다. 지난해 초판 발행 이후 데이터 영구화, 중산층 일자리, 자유와 안보, 디지털 혁명 문제 등을 추가한 개정 증보판이다.(이진원 옮김, 알키, 520쪽, 2만원)◇종횡무진 역사=동양과 서양 문명을 비교하며 두 문명의 탄생, 만남, 융합을 그린 책. 과거 문명 탐구부터 문명의 발전 방향까지 살폈다. 책 이름에 맞게 일방통행적 역사 서술이 아니라 시·공간을 넘나들며 역사 현장을 누빈다. 역사를 현실과 함께 이해하려는 저자의 문제의식이 돋보인다.(남경태 지음, 휴머니스트, 744쪽, 3만8000원)◇영원의 철학=‘영원의 철학’은 16세기에 처음 언급된 ‘모든 위대한 종교의 본질적이고 공통된 핵심 진리’로, 세계의 종교적 전통이 공유하는 세계관, 인간관, 윤리관을 말한다.《멋진 신세계》의 올더스 헉슬리가 420개의 동서고금 문헌을 통해 영원의 철학을 소개한다. 인용문만 읽어도 흥미로운 종교서이자 명상서.(조옥경 옮김, 김영사, 528쪽, 1만9800원)◇대한민국 5천년 역사리더십을 말한다=고조선의 단군부터 이명박 전 대통령에 이르는 우리 민족의 리더십을 다룬 책. 리더십의 관점에서 우리 역사를 바라보며 역사 속 사건들에서 리더십이 어떤 힘을 발휘했는지 살펴보고 홍익인간, 민족주의, 민주주의 등 우리 민족 고유의 리더십 DNA 여덟 가지를 제시한다.(최익용 지음, 옥당, 436쪽, 2만2000원)◇한국 자본주의 모델=진보적 한국 경제론의 이론과 참여를 주도해 온 저자가 광복 이후 박근혜 정부까지 70여년의 대한민국 경제·자본주의 모델을 분석한 책. 이승만 정부의 대미 의존 경제, 박정희 개발주의의 명암을 살펴보면서 민주화 이후의 한국 자본주의와 외환위기 후 경제민주화와 한국 경제의 전망을 탐색한다.(이병천 지음, 책세상, 480쪽, 2만5000원)\n",
      "-----\n",
      "문서5: “한국이 이뤄낸 ‘한강의 기적’이야말로 빈곤과 질병으로부터의 ‘위대한 탈출(the great escape)’이다.”오정근 한국경제연구원 초빙연구위원은 3일 서울 태평로 프레스센터에서 열린 ‘앵거스 디턴 《위대한 탈출》의 의의와 한국 경제에 주는 시사점’이란 토론회에서 이같이 말했다. 그는 “디턴 미 프린스턴대 교수는 《위대한 탈출》이란 책을 통해 인류가 산업혁명 후 250여년간 비약적인 경제성장으로 소득 수준이 높아지고 기대수명이 길어져 삶의 질, 즉 웰빙(well-being) 수준이 크게 개선되는 대탈출을 달성했다고 강조했다”며 “한국은 1960년대 이후 경제성장을 통해 이런 탈출을 이뤄냈다”고 설명했다.경제개발 5개년 계획이 시작됐던 1962년 한국의 1인당 국내총생산(GDP)은 90달러로 미얀마의 45% 수준이었다. 지난해 1인당 GDP는 2만7963달러로 52년 동안 311배 증가했다.고도성장을 통해 위대한 탈출을 이뤄냈지만 성장률이 둔화하면서 분배도 악화됐다는 것이 그의 분석이다. 오 위원은 “1987년 노동자 대투쟁 이후 6년간 연평균 임금상승률이 20%를 기록하는 등 고임금-저생산이 고착화됨에 따라 한국 기업이 해외로 탈출하기 시작했다”며 “그 결과 한국은 1992년을 기점으로 ‘중(中)성장 시대’를 맞게 됐다”고 설명했다. 1963~1991년 연평균 9.5%였던 한국의 경제성장률은 1992~2011년 5.1%로 하락했다. 올해 성장률은 2%대를 기록할 것으로 전망된다.그는 “경제성장률이 1% 높아질 때 소득 분배의 불평등도를 나타내는 지니계수는 0.3% 감소하는 것으로 나타났다”며 “성장률 침체로 지니계수가 높아지고 중산층이 몰락하는 등 분배구조가 악화일로를 걷기 시작했다”고 분석했다. 1990년 0.266이었던 지니계수는 지난해 0.308로 악화했다.오 위원은 “하락하고 있는 경제성장률을 반등시켜 선진국으로 도약하기 위해서는 구조개혁과 규제혁파로 투자를 활성화하고 고부가가치 서비스산업을 육성하는 등 성장동력 확충에 최선을 다해야 한다”며 “무엇보다 중요한 것은 다시 한번 일어서겠다는 대탈출에 대한 의지”라고 강조했다.<|im_end|>\n",
      "<|im_start|>user\n",
      "한국 경제의 성장과 분배 문제에 대해 설명해 주세요.<|im_end|>\n",
      "<|im_start|>assistant\n"
     ]
    }
   ],
   "source": [
    "print(prompt_lst[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6dc4669f-bc8a-4071-a80f-cd637b0806a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "한국 경제는 지난 수십 년간 눈부신 성장을 이루어냈지만, 이와 동시에 분배 문제도 심각하게 대두되고 있습니다. \n",
      "\n",
      "한국은 1960년대 이후 경제개발 5개년 계획을 통해 급격한 경제 성장을 이루었으며, 이를 통해 '한강의 기적'이라 불리는 경제적 도약을 경험했습니다. 1962년 한국의 1인당 국내총생산(GDP)은 90달러에 불과했으나, 2014년에는 2만7963달러로 311배 증가했습니다. 이러한 고도성장은 한국이 빈곤과 질병으로부터 벗어나는 데 큰 기여를 했습니다 [[ref5]].\n",
      "\n",
      "그러나 경제 성장률이 둔화되면서 분배 문제도 심각해졌습니다. 1987년 노동자 대투쟁 이후 고임금-저생산 구조가 고착화되면서 한국 기업들이 해외로 이전하기 시작했고, 이는 경제 성장률의 둔화로 이어졌습니다. 1963년부터 1991년까지 연평균 9.5%였던 경제 성장률은 1992년부터 2011년까지 5.1%로 하락했습니다. 최근에는 2%대 성장률을 기록할 것으로 전망되고 있습니다 [[ref5]].\n",
      "\n",
      "경제 성장률의 둔화는 소득 분배의 불평등을 악화시켰습니다. 경제 성장률이 1% 높아질 때 소득 분배의 불평등도를 나타내는 지니계수는 0.3% 감소하는 것으로 나타났지만, 성장률 침체로 인해 지니계수가 높아지고 중산층이 몰락하는 등 분배 구조가 악화되었습니다. 1990년 0.266이었던 지니계수는 2014년 0.308로 악화되었습니다 [[ref5]].\n",
      "\n",
      "따라서 한국 경제가 다시 성장하기 위해서는 구조개혁과 규제 혁파로 투자를 활성화하고, 고부가가치 서비스 산업을 육성하는 등 성장 동력을 확충하는 것이 필요합니다. 무엇보다 중요한 것은 다시 한번 경제적 도약을 이루겠다는 의지입니다 [[ref5]].<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(label_lst[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "927909d6-b3d8-49e6-9659-79f6e328fe9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bb9aed61a74fe2bc61a446e741fe76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 예측:\n",
      "한국 경제의 성장과 분배 문제에 대한 설명을 제공하기 위해 여러 문서를 참조하였습니다.\n",
      "\n",
      "한국은 1960년대 이후 경제성장을 통해 '위대한 탈출'을 이뤄냈습니다. 이는 인류가 산업혁명 이후 250여년간의 비약적인 경제성장으로 소득 수준이 높아지고 기대수명이 길어져 삶의 질이 크게 개선되는 것을 의미합니다. 1962년 한국의 1인당 국내총생산(GDP)은 90달러로 미얀마의 45% 수준이었지만, 2019년에는 2만7963달러로 52년 동안 311배 증가했습니다.\n",
      "\n",
      "그러나 성장률이 둔화되면서 분배도 악화되었습니다. 1987년 노동자 대투쟁 이후 고임금-저생산이 고착화되어 한국 기업이 해외로 탈출하기 시작했습니다. 이로 인해 한국은 1992년을 기점으로 '중성장 시대'를 맞게 되었습니다. 1963~1991년 연평균 9.5%였던 한국의 경제성장률은 1992~2011년 5.1%로 하락했습니다. 2022년의 성장률은 2%대를 기록할 것으로 전망됩니다.\n",
      "\n",
      "이렇게 경제성장률이 떨어지면서 소득 분배의 불평등도가 높아졌습니다. 1990년 0.266이었던 지니계수는 2019년 0.308로 악화되었습니다. \n",
      "\n",
      "이러한 문제를 해결하기 위해서는 구조개혁과 규제혁파로 투자를 활성화하고 고부가가치 서비스산업을 육성하는 등 성장동력 확충에 최선을 다해야 합니다. 무엇보다 중요한 것은 다시 한번 일어서겠다는 대탈출에 대한 의지가 필요하다는 것이 한국 경제에 대한 분석입니다.\n",
      "정답:\n",
      "\n",
      "한국 경제는 지난 수십 년간 눈부신 성장을 이루어냈지만, 이와 동시에 분배 문제도 심각하게 대두되고 있습니다. \n",
      "\n",
      "한국은 1960년대 이후 경제개발 5개년 계획을 통해 급격한 경제 성장을 이루었으며, 이를 통해 '한강의 기적'이라 불리는 경제적 도약을 경험했습니다. 1962년 한국의 1인당 국내총생산(GDP)은 90달러에 불과했으나, 2014년에는 2만7963달러로 311배 증가했습니다. 이러한 고도성장은 한국이 빈곤과 질병으로부터 벗어나는 데 큰 기여를 했습니다 [[ref5]].\n",
      "\n",
      "그러나 경제 성장률이 둔화되면서 분배 문제도 심각해졌습니다. 1987년 노동자 대투쟁 이후 고임금-저생산 구조가 고착화되면서 한국 기업들이 해외로 이전하기 시작했고, 이는 경제 성장률의 둔화로 이어졌습니다. 1963년부터 1991년까지 연평균 9.5%였던 경제 성장률은 1992년부터 2011년까지 5.1%로 하락했습니다. 최근에는 2%대 성장률을 기록할 것으로 전망되고 있습니다 [[ref5]].\n",
      "\n",
      "경제 성장률의 둔화는 소득 분배의 불평등을 악화시켰습니다. 경제 성장률이 1% 높아질 때 소득 분배의 불평등도를 나타내는 지니계수는 0.3% 감소하는 것으로 나타났지만, 성장률 침체로 인해 지니계수가 높아지고 중산층이 몰락하는 등 분배 구조가 악화되었습니다. 1990년 0.266이었던 지니계수는 2014년 0.308로 악화되었습니다 [[ref5]].\n",
      "\n",
      "따라서 한국 경제가 다시 성장하기 위해서는 구조개혁과 규제 혁파로 투자를 활성화하고, 고부가가치 서비스 산업을 육성하는 등 성장 동력을 확충하는 것이 필요합니다. 무엇보다 중요한 것은 다시 한번 경제적 도약을 이루겠다는 의지입니다 [[ref5]].<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "base_model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "eos_token = tokenizer(\"<|im_end|>\",add_special_tokens=False)[\"input_ids\"][0]\n",
    "\n",
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()\n",
    "\n",
    "prompt=prompt_lst[1000]\n",
    "label=label_lst[1000]\n",
    "pred=test_inference(pipe, prompt)\n",
    "\n",
    "print(f\"모델의 예측:\\n{test_inference(pipe, prompt)}\")\n",
    "print(f\"정답:\\n{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ea6a68f-b3d3-46ef-998d-317b86853ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b870ef5454f4421db7fbed8885395b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 예측:\n",
      "한국 경제의 성장과 분배 문제는 여러 가지 요인으로 인해 발생하고 있습니다. 한국은 1960년대 이후 경제성장을 통해 \"위대한 탈출\"을 이뤄냈습니다. 이는 인류가 산업혁명 후 250여년간 비약적인 경제성장으로 소득 수준이 높아지고 기대수명이 길어져 삶의 질이 크게 개선된 것을 의미합니다[[ref5]].\n",
      "\n",
      "그러나 이러한 성장은 분배 문제를 야기했습니다. 1987년 노동자 대투쟁 이후, 고임금-저생산이 고착화되면서 한국 기업들은 해외로 탈출하기 시작했습니다. 이로 인해 한국은 1992년을 기점으로 \"중성장 시대\"를 맞게 되었으며, 경제성장률이 하락했습니다. 1963~1991년 연평균 9.5%였던 한국의 경제성장률은 1992~2011년 5.1%로 하락했습니다. 2012년 이후에도 성장률은 2%대를 기록하고 있습니다[[ref5]].\n",
      "\n",
      "경제성장률이 둔화됨에 따라 분배도 악화되었습니다. 1990년 0.266이었던 지니계수는 2012년 0.308로 악화되었습니다. 이는 경제성장률이 높아질 때 소득 분배의 불평등도를 나타내는 지니계수는 0.3% 감소하는 것으로 나타났음을 의미합니다[[ref5]].\n",
      "\n",
      "따라서 한국 경제의 성장과 분배 문제를 해결하기 위해서는 구조개혁과 규제혁파로 투자를 활성화하고 고부가가치 서비스산업을 육성하는 등 성장동력 확충에 최선을 다해야 합니다. 또한, 다시 한번 일어서겠다는 대탈출에 대한 의지가 필요합니다[[ref5]].\n",
      "정답:\n",
      "\n",
      "한국 경제는 지난 수십 년간 눈부신 성장을 이루어냈지만, 이와 동시에 분배 문제도 심각하게 대두되고 있습니다. \n",
      "\n",
      "한국은 1960년대 이후 경제개발 5개년 계획을 통해 급격한 경제 성장을 이루었으며, 이를 통해 '한강의 기적'이라 불리는 경제적 도약을 경험했습니다. 1962년 한국의 1인당 국내총생산(GDP)은 90달러에 불과했으나, 2014년에는 2만7963달러로 311배 증가했습니다. 이러한 고도성장은 한국이 빈곤과 질병으로부터 벗어나는 데 큰 기여를 했습니다 [[ref5]].\n",
      "\n",
      "그러나 경제 성장률이 둔화되면서 분배 문제도 심각해졌습니다. 1987년 노동자 대투쟁 이후 고임금-저생산 구조가 고착화되면서 한국 기업들이 해외로 이전하기 시작했고, 이는 경제 성장률의 둔화로 이어졌습니다. 1963년부터 1991년까지 연평균 9.5%였던 경제 성장률은 1992년부터 2011년까지 5.1%로 하락했습니다. 최근에는 2%대 성장률을 기록할 것으로 전망되고 있습니다 [[ref5]].\n",
      "\n",
      "경제 성장률의 둔화는 소득 분배의 불평등을 악화시켰습니다. 경제 성장률이 1% 높아질 때 소득 분배의 불평등도를 나타내는 지니계수는 0.3% 감소하는 것으로 나타났지만, 성장률 침체로 인해 지니계수가 높아지고 중산층이 몰락하는 등 분배 구조가 악화되었습니다. 1990년 0.266이었던 지니계수는 2014년 0.308로 악화되었습니다 [[ref5]].\n",
      "\n",
      "따라서 한국 경제가 다시 성장하기 위해서는 구조개혁과 규제 혁파로 투자를 활성화하고, 고부가가치 서비스 산업을 육성하는 등 성장 동력을 확충하는 것이 필요합니다. 무엇보다 중요한 것은 다시 한번 경제적 도약을 이루겠다는 의지입니다 [[ref5]].<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"qwen2-7b-rag-ko/checkpoint-285\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)\n",
    "\n",
    "prompt=prompt_lst[1000]\n",
    "label=label_lst[1000]\n",
    "pred=test_inference(pipe, prompt)\n",
    "\n",
    "print(f\"모델의 예측:\\n{test_inference(pipe, prompt)}\")\n",
    "print(f\"정답:\\n{label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
